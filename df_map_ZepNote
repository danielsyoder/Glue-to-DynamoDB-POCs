#Zepplin Notebook to test Glue-to-DynamoDB transfer job
# https://docs.aws.amazon.com/glue/latest/dg/dev-endpoint-notebook-server-considerations.html
#In addition to role permissions, the notebook requires a DynamoDB VPCe to run.

import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark.sql.functions import udf
import boto3
#import threading
#import uuid

glueContext = GlueContext(sc)
sourceTable = "glueSourceTable"

#Set up DynamicFrame in Glue and map sourceTable
datasource0 = glueContext.create_dynamic_frame.from_catalog(database = "default", table_name = sourceTable, transformation_ctx = "datasource0")
applymapping1 = ApplyMapping.apply(frame = datasource0, mappings = [("col0", "string", "col0", "string"), ("col1", "string", "col1", "string")], transformation_ctx = "applymapping1")

#Move DynamicFrame to DataFrame
dfMap = applymapping1.toDF()

##Function to be called against each row.
def rowToDDB(col0,col1):
    #Set mapping between columns & DynamoDB table attributes
    ddbObj = {
        "k":{"S":col1},
        "Op":{"S":col0}
        }
    
    #Initialize client & set table environment is required within the loop due to threads/locking.
    client = boto3.client("dynamodb", region_name="us-east-1")
    ddbTableOut = "ddbOutputTable"    
    client.put_item(TableName=ddbTableOut, Item=ddbObj)

##Set up User Defined Function to transform DataFrame 
row_udf = udf(lambda col0,col1: rowToDDB(col0,col1))

##Map each row to the DynamoDB PUT api function --> .collect() to run each row-put function
dfMap.select(row_udf('col0','col1')).collect()
